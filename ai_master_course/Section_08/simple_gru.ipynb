{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"simple_gru.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"mHrqEf0hTKvn","colab_type":"text"},"source":["# シンプルなGRUの実装\n","GRU層を用いてシンプルなニューラルネットワークを構築し、時系列データを学習します。  \n","今回は、LSTMとGRUを比較します。"]},{"cell_type":"markdown","metadata":{"id":"AJBTRnQJTKvq","colab_type":"text"},"source":["## 訓練用データの作成\n","訓練用のデータを作成します。  \n","サイン関数に乱数でノイズを加えたデータを作成し、過去の時系列データから未来の値を予測できるようにします。  \n","これまでと同様に、正解は入力の時系列を一つ後にずらしたものにします。"]},{"cell_type":"code","metadata":{"id":"xON6wgtNTKvr","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","x_data = np.linspace(-2*np.pi, 2*np.pi)  # -2πから2πまで\n","sin_data = np.sin(x_data) + 0.1*np.random.randn(len(x_data))  # sin関数に乱数でノイズを加える\n","\n","plt.plot(x_data, sin_data)\n","plt.show()\n","\n","n_rnn = 10  # 時系列の数\n","n_sample = len(x_data)-n_rnn  # サンプル数\n","x = np.zeros((n_sample, n_rnn))  # 入力\n","t = np.zeros((n_sample, n_rnn))  # 正解\n","for i in range(0, n_sample):\n","    x[i] = sin_data[i:i+n_rnn]\n","    t[i] = sin_data[i+1:i+n_rnn+1]  # 時系列を入力よりも一つ後にずらす\n","\n","x = x.reshape(n_sample, n_rnn, 1)  # サンプル数、時系列の数、入力層のニューロン数\n","print(x.shape)\n","t = t.reshape(n_sample, n_rnn, 1)  # 今回は入力と同じ形状\n","print(t.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VnZYKq3_TKvu","colab_type":"text"},"source":["## LSTMとGRUの比較\n","Kerasを使ってLSTM、およびGRUを構築します。  \n","Kerasにおいて、GRU層はSimpleRNN層やLSTM層と同じ方法で追加することができます。"]},{"cell_type":"code","metadata":{"id":"Cynv_bSHTKvv","colab_type":"code","colab":{}},"source":["from keras.models import Sequential\n","from keras.layers import Dense, LSTM, GRU\n","\n","batch_size = 8  # バッチサイズ\n","n_in = 1  # 入力層のニューロン数\n","n_mid = 20  # 中間層のニューロン数\n","n_out = 1  # 出力層のニューロン数\n","\n","# 比較のためのLSTM\n","model_lstm = Sequential()\n","model_lstm.add(LSTM(n_mid, input_shape=(n_rnn, n_in), return_sequences=True))\n","model_lstm.add(Dense(n_out, activation=\"linear\"))\n","model_lstm.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n","print(model_lstm.summary())\n","\n","# GRU\n","model_gru = Sequential()\n","model_gru.add(GRU(n_mid, input_shape=(n_rnn, n_in), return_sequences=True))\n","model_gru.add(Dense(n_out, activation=\"linear\"))\n","model_gru.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n","print(model_gru.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9NqvcuNSTKvy","colab_type":"text"},"source":["LSTMよりもGRUの方がモデルがシンプルなため、パラメータが少ないことが確認できます。"]},{"cell_type":"markdown","metadata":{"id":"jDjWZDfATKvz","colab_type":"text"},"source":["## 学習\n","構築したRNNのモデルを使って、学習を行います。  "]},{"cell_type":"code","metadata":{"id":"0rgIMBC2TKvz","colab_type":"code","colab":{}},"source":["import time\n","\n","epochs = 1000\n","\n","# LSTM\n","start_time = time.time()\n","history_lstm = model_lstm.fit(x, t, epochs=epochs, batch_size=batch_size, verbose=0)\n","print(\"学習時間 --LSTM--:\", time.time() - start_time)\n","\n","# GRU\n","start_time = time.time()\n","history_gru = model_gru.fit(x, t, epochs=epochs, batch_size=batch_size, verbose=0)\n","print(\"学習時間 --GRU--:\", time.time() - start_time)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ew8bSyKETKv2","colab_type":"text"},"source":["エポック数が同じ場合、パラメータ数が少ないためGRUの方が学習に時間がかかりません。"]},{"cell_type":"markdown","metadata":{"id":"qxuF6EpNTKv2","colab_type":"text"},"source":["## 学習の推移\n","誤差の推移を確認します。"]},{"cell_type":"code","metadata":{"id":"lX0QBjw7TKv3","colab_type":"code","colab":{}},"source":["loss_lstm = history_lstm.history['loss']\n","loss_gru = history_gru.history['loss']\n","\n","plt.plot(np.arange(len(loss_lstm)), loss_lstm, label=\"LSTM\")\n","plt.plot(np.arange(len(loss_gru)), loss_gru, label=\"GRU\")\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-pVbZYFNTKv5","colab_type":"text"},"source":["LSTMと比較して、GRUの方が早く収束します。"]},{"cell_type":"markdown","metadata":{"id":"-vgWmLvMTKv6","colab_type":"text"},"source":["## 学習済みモデルの使用\n","それぞれの学習済みモデルを使って、サイン関数の次の値を予測します。"]},{"cell_type":"code","metadata":{"id":"hak1H_JtTKv7","colab_type":"code","colab":{}},"source":["predicted_lstm = x[0].reshape(-1) \n","predicted_gru = x[0].reshape(-1) \n","\n","for i in range(0, n_sample):\n","    y_lstm = model_lstm.predict(predicted_lstm[-n_rnn:].reshape(1, n_rnn, 1))\n","    predicted_lstm = np.append(predicted_lstm, y_lstm[0][n_rnn-1][0])\n","    y_gru = model_gru.predict(predicted_gru[-n_rnn:].reshape(1, n_rnn, 1))\n","    predicted_gru = np.append(predicted_gru, y_gru[0][n_rnn-1][0])\n","\n","plt.plot(np.arange(len(sin_data)), sin_data, label=\"Training data\")\n","plt.plot(np.arange(len(predicted_lstm)), predicted_lstm, label=\"Predicted_LSTM\")\n","plt.plot(np.arange(len(predicted_gru)), predicted_gru, label=\"Predicted_GRU\")\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mfr0ZZkrTKv9","colab_type":"text"},"source":["GRUを使ったモデルも、シンプルなRNNやLSTMと同様にサインカーブを学習できていることが分かります。  \n","このグラフでは、GRUの方がよく収束しています。\n","\n","このように、GRUはパラメータ数が少ないため1エポックに必要な時間は短く、LSTMよりも早く収束する傾向があります。  \n","ただ、LSTMの方が複雑な時系列の学習に向いているケースもあるので、状況に応じてLSTMとGRUを使い分ける必要があります。"]}]}