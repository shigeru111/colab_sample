{"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"K74mqS_L8nq4"},"source":["# 深層強化学習の実装\n","ディープラーニングを利用したQ学習を実装します。Q学習は強化学習の一種ですが、今回はニューラルネットワークを使って状態からQ値を求めます。Deep Q-Network（DQN）に近いですが、それよりも簡単な実装になります。  \n","今回は、強化学習により重力下で飛行する物体の制御を行います。\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3is6apnw8SIr"},"source":["## 各設定\n","tensorflowとKerasのバージョンによっては、Kerasのコードでエラーが発生することがあります。エラーを回避するために、以下のセルでKerasのバージョンを指定してインストールします。  \n","以下のコードは、デフォルトのバージョンでエラーが発生しないときには必要ありません。"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"XGerSny2H-Pf","vscode":{"languageId":"python"}},"outputs":[],"source":["!pip install tensorflow==2.9.1\n","!pip install keras==2.9.0"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"NolPkrDiferU"},"source":["なお、Googleの対応により上記のコードは近いうちに必要なくなるかと思います。  \n","今後tensorflowやKerasのバージョンアップにより同様の問題が発生する可能性がありますが、上記のようにしてバージョンを調整することによる対応が必要になります。  \n","上記のセルの実行後、**ランタイム→ランタイムを再起動**によりバージョンの更新が完了します。  \n","念のために、以下のコードによりバージョンを確認しておきましょう。"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"1RIp0qHaIQ9i","vscode":{"languageId":"python"}},"outputs":[],"source":["import tensorflow\n","import keras\n","print(tensorflow.__version__)\n","print(keras.__version__)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ttmC5S-EaYLA"},"source":["必要なモジュールのインポート、及び最適化アルゴリズムの設定を行います。"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"n6Ts3siI8awo","vscode":{"languageId":"python"}},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt \n","from matplotlib import animation, rc\n","\n","from keras.models import Sequential\n","from keras.layers import Dense, ReLU\n","from keras.optimizers import RMSprop\n","\n","optimizer = RMSprop()"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"dsBqQ3dG4fcf"},"source":["### Brainクラス\n","エージェントの頭脳となるクラスです。Q値を出力するニューラルネットワークを構築し、Q値が正解に近づくように訓練します。  \n","Q学習に用いる式は以下の通りです。  \n","\n","$$ Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\eta\\left(R_{t+1}+\\gamma \\max_{a}Q(s_{t+1}, a) - Q(s_{t}, a_{t})\\right) $$\n","\n","ここで、$a_{t}$は行動、$s_t$は状態、$Q(s_t,a_t) $はQ値、$\\eta$は学習係数、$R_{t+1}$は報酬、$\\gamma$は割引率になります。  \n","次の状態における最大のQ値を使用するのですが、ディープラーニングの正解として用いるのは上記の式のうちの以下の部分です。  \n","\n","$$R_{t+1}+\\gamma \\max_{a}Q(s_{t+1}, a_{t})$$\n","\n","以下の`Brain`クラスにおけるtrainメソッドでは、正解として上記を用います。  \n","また、ある状態における行動を決定する`get_action`メソッドでは、ε-greedy法により行動が選択されます。\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"AFZjkmS_RiLx","vscode":{"languageId":"python"}},"outputs":[],"source":["class Brain:\n","    def __init__(self, n_state, n_mid, n_action, gamma=0.9, r=0.99):\n","        self.eps = 1.0  # ε\n","        self.gamma = gamma  # 割引率\n","        self.r = r  # εの減衰率\n","\n","        model = Sequential()\n","        model.add(Dense(n_mid, input_shape=(n_state,)))\n","        model.add(ReLU()) \n","        model.add(Dense(n_mid))\n","        model.add(ReLU()) \n","        model.add(Dense(n_action))\n","        model.compile(loss=\"mse\", optimizer=optimizer)\n","        self.model = model\n","\n","    def train(self, states, next_states, action, reward, terminal):\n","        q = self.model.predict(states)  \n","        next_q = self.model.predict(next_states)\n","        t = np.copy(q)\n","        if terminal:\n","            t[:, action] = reward  #  エピソード終了時の正解は、報酬のみ\n","        else:\n","            t[:, action] = reward + self.gamma*np.max(next_q, axis=1)\n","        self.model.train_on_batch(states, t)\n","\n","    def get_action(self, states):\n","        q = self.model.predict(states)\n","        if np.random.rand() < self.eps:\n","            action = np.random.randint(q.shape[1], size=q.shape[0])\n","        else:\n","            action = np.argmax(q, axis=1)\n","        if self.eps > 0.1:  # εの下限\n","            self.eps *= self.r\n","        return action"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"b80LdWEVgwI5"},"source":["### エージェントのクラス\n","エージェントをクラスとして実装します。  \n","x座標が-1から1まで、y座標が-1から1までの正方形の領域を考えますが、エージェントの初期位置は左端中央とします。  \n","そして、エージェントが右端に達した際は報酬として1を与え、終了とします。また、エージェントが上端もしくは下端に達した際は報酬として-1を与え、終了とします。上手く飛行できた場合は褒美を、失敗した際は罰を与えるイメージです。  \n","行動には、自由落下とジャンプの2種類があります。自由落下の場合は重量加速度をy速度に加えます。ジャンプの場合は、y速度を予め設定した値に変更します。"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"lTkYy_4jYKlj","vscode":{"languageId":"python"}},"outputs":[],"source":["class Agent:\n","    def __init__(self, v_x, v_y_sigma, v_jump, brain):\n","        self.v_x = v_x  # x速度\n","        self.v_y_sigma = v_y_sigma  # y速度、初期値の標準偏差\n","        self.v_jump = v_jump  # ジャンプ速度\n","        self.brain = brain\n","        self.reset()\n","\n","    def reset(self):\n","        self.x = -1  # 初期x座標\n","        self.y = 0  # 初期y座標\n","        self.v_y = self.v_y_sigma * np.random.randn()  # 初期y速度\n","\n","    def step(self, g):  # 時間を1つ進める g:重力加速度\n","        states = np.array([[self.y, self.v_y]])\n","        self.x += self.v_x\n","        self.y += self.v_y\n","\n","        reward = 0  # 報酬\n","        terminal = False  # 終了判定\n","        if self.x>1.0:\n","            reward = 1\n","            terminal = True\n","        elif self.y<-1.0 or self.y>1.0:\n","            reward = -1\n","            terminal = True\n","        reward = np.array([reward])\n","\n","        action = self.brain.get_action(states)\n","        if action[0] == 0:\n","            self.v_y -= g   # 自由落下\n","        else:\n","            self.v_y = self.v_jump  # ジャンプ\n","        next_states = np.array([[self.y, self.v_y]])\n","        self.brain.train(states, next_states, action, reward, terminal)\n","\n","        if terminal:\n","            self.reset()"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"AAYDnNlPJ46Z"},"source":["## 環境のクラス\n","環境をクラスとして実装します。このクラスの役割は、重力加速度を設定し、時間を前に進めるのみです。"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"EcKpe2k38bax","vscode":{"languageId":"python"}},"outputs":[],"source":["class Environment:\n","    def __init__(self, agent, g):\n","        self.agent = agent\n","        self.g = g\n","\n","    def step(self):\n","        self.agent.step(self.g)\n","        return (self.agent.x, self.agent.y)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-esctn70T1mO"},"source":["## アニメーション\n","今回は、matplotlibを使って物体の飛行をアニメーションで表します。  \n","アニメーションには、matplotlib.animationのFuncAnimation関数を使用します。  "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"A1JRoW5HS41A","vscode":{"languageId":"python"}},"outputs":[],"source":["def animate(environment, interval, frames):\n","    fig, ax = plt.subplots()\n","    plt.close()\n","    ax.set_xlim(( -1, 1))\n","    ax.set_ylim((-1, 1))\n","    sc = ax.scatter([], [])\n","\n","    def plot(data):\n","        x, y = environment.step()\n","        sc.set_offsets(np.array([[x, y]]))\n","        return (sc,)\n","\n","    return animation.FuncAnimation(fig, plot, interval=interval, frames=frames, blit=True)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VOab_hMrknSM"},"source":["## ランダムな行動\n","まずは、エージェントがランダムに行動する例をみていきましょう。`r`の値を1に設定し、εが減衰しないようにすることで、エージェントは完全にランダムな行動を選択するようになります。"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"gI6ZhfEYUQnf","vscode":{"languageId":"python"}},"outputs":[],"source":["n_state = 2\n","n_mid = 32\n","n_action = 2\n","brain = Brain(n_state, n_mid, n_action, r=1.0)  # εが減衰しない\n","\n","v_x = 0.05\n","v_y_sigma = 0.1\n","v_jump = 0.2\n","agent = Agent(v_x, v_y_sigma, v_jump, brain)\n","\n","g = 0.2\n","environment = Environment(agent, g)\n","\n","anim = animate(environment, 50, 1024)\n","rc('animation', html='jshtml')\n","anim"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"IPbPx-IVlH4E"},"source":["運良く右端に到達することもありますが、大抵は上端もしくは下端にぶつかってしまいます。"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Nns_zxAllcDv"},"source":["## Q学習の導入\n","`r`の値を0.99に設定し、εが減衰するようにします。これにより、次第にQ学習が行われるようになります。"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"eF8qCWnMYLhl","vscode":{"languageId":"python"}},"outputs":[],"source":["n_state = 2\n","n_mid = 32\n","n_action = 2\n","brain = Brain(n_state, n_mid, n_action, r=0.99)  # εが減衰する\n","\n","v_x = 0.05\n","v_y_sigma = 0.1\n","v_jump = 0.2\n","agent = Agent(v_x, v_y_sigma, v_jump, brain)\n","\n","g = 0.2\n","environment = Environment(agent, g)\n","\n","anim = animate(environment, 50, 1024)\n","rc('animation', html='jshtml')\n","anim"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"OetFBzcZkvSB"},"source":["学習が進むと、上下の端にぶつらずに飛べるようになります。  \n","なお、初期条件によっては学習に失敗することもあります。  "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"1mL2pVbqoh_E"},"source":["## DQNのテクニック\n","Deep Q-Network（DQN）では、一般的に以下のテクニックが使われます。\n","\n","### Experience Replay\n","各ステップの内容をメモリに保存しておき、メモリからランダムに記録を取り出して学習を行います。  \n","これにより、ミニバッチ法を使用することが可能になります。  \n","また、ミニバッチに時間的にばらけた記録が入ることになるので、学習が安定します。  \n","\n","### Fixed Target Q-Network\n","行動を決定するのに用いるmain-networkと、誤差の計算時に正解を求めるのに用いるtarget-networkを用意します。  \n","target-networkのパラメータは一定時間固定されますが、main-networkのパラメータはミニバッチごとに更新されます。そして、定期的にmain-networkのパラメータがtarget-networkに上書きされます。これにより、正解が短い時間で揺れ動く問題が低減され学習が安定すると考えられています。  \n","\n"]}],"metadata":{"colab":{"collapsed_sections":[],"name":" deep_reinforcement_learning.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
