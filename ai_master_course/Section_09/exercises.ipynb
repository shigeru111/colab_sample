{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"exercises.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"K74mqS_L8nq4","colab_type":"text"},"source":["# 演習\n","VAE（変分自己符号器）のモデルを構築しましょう。"]},{"cell_type":"markdown","metadata":{"id":"AAYDnNlPJ46Z","colab_type":"text"},"source":["## 訓練用データの用意\n","VAEに用いる訓練用のデータを用意します。  \n","MNIST（手書き文字）のデータを読み込み、表示します。"]},{"cell_type":"code","metadata":{"id":"EcKpe2k38bax","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt \n","from keras.datasets import mnist\n","\n","(x_train, t_train), (x_test, t_test) = mnist.load_data()  # MNISTの読み込み\n","print(x_train.shape, x_test.shape)  # 28x28の手書き文字画像が6万枚\n","\n","# 各ピクセルの値を0-1の範囲に収める\n","x_train = x_train / 255\n","x_test = x_test / 255\n","\n","# 手書き文字画像の表示\n","plt.imshow(x_train[0].reshape(28, 28), cmap=\"gray\")\n","plt.title(t_train[0])\n","plt.show() \n","\n","# 一次元に変換する\n","x_train = x_train.reshape(x_train.shape[0], -1)\n","x_test = x_test.reshape(x_test.shape[0], -1)\n","print(x_train.shape, x_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3is6apnw8SIr","colab_type":"text"},"source":["## VAEの各設定\n","VAEに必要な各設定を行います。"]},{"cell_type":"code","metadata":{"id":"n6Ts3siI8awo","colab_type":"code","colab":{}},"source":["epochs = 10  \n","batch_size = 128\n","n_in_out = 784  # 入出力層のニューロン数\n","n_z = 2  # 潜在変数の数（次元数）\n","n_mid = 256  # 中間層のニューロン数"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QdzmlSE_w7bD","colab_type":"text"},"source":["## モデルの構築\n","以下のセルにコードを追記し、VAEのモデルを完成させましょう。  \n","**Reparametrization Trick**、および**損失関数**のコードを記述してください。  \n","  \n"," Encoderの出力は、潜在変数の平均値$\\mu$および、標準偏差$\\sigma$の2乗（=分散）の対数とします。  \n"," 出力に標準偏差ではなく分散の対数を使うことで、出力範囲が狭くても広い範囲の標準偏差を表すことができます。   \n","   \n","VAEのコードでは、バックプロパゲーションによる学習のためにReparametrization Trickが使われます。    \n","平均値0標準偏差1のノイズεを発生させて、標準偏差$\\sigma$とかけて平均値$\\mu$に加えることで、潜在変数$z$とします。  \n","$$\\epsilon \\sim N(0,\\, I)$$\n","$$z=\\mu + \\epsilon\\odot\\sigma$$\n","  \n","損失関数についてですが、以下の式で表される項は出力がどれだけ入力を再現できているかを表します。  \n","$$\\frac{1}{L}\\sum_{i=1}^{L}\\sum_{j=1}^{D}x_{ij}\\log y_{ij}+(1-x_{ij})log(1-y_{ij})$$\n","$L$:バッチサイズ、￼$D$:入力層のニューロン数、￼$x_{ij}$: VAEの入力、$y_{ij}$: VAEの出力   \n","また、以下の式で表される項は、平均値が0に、標準偏差が1に近づくように機能します。  \n","$$\\frac{1}{2}\\sum_{k=1}^{J}(1+\\log \\sigma_k^2 - \\mu_k^2 - \\sigma_k^2)$$  \n","$J$: 潜在変数の次元数  \n","VAEでは、これらの項の和を損失関数として使用します。"]},{"cell_type":"code","metadata":{"id":"Ql5XWTfVWghn","colab_type":"code","colab":{}},"source":["from keras.models import Model\n","from keras import metrics  # 評価関数\n","from keras.layers import Input, Dense, Lambda\n","from keras import backend as K  # 乱数の発生に使用\n","\n","# 潜在変数をサンプリングするための関数\n","def z_sample(args):\n","    mu, log_var = args  # 潜在変数の平均値と、分散の対数\n","    # ---- ここからReparametrization Trickのコードを書く ---\n","\n","    return \n","    # ---- ここまで ---\n","\n","# VAEのネットワーク構築\n","x = Input(shape=(n_in_out,))\n","h_encoder = Dense(n_mid, activation=\"relu\")(x)\n","\n","mu = Dense(n_z)(h_encoder)\n","log_var = Dense(n_z)(h_encoder)\n","z = Lambda(z_sample, output_shape=(n_z,))([mu, log_var])\n","\n","mid_decoder = Dense(n_mid, activation=\"relu\")  # 後で使用\n","h_decoder = mid_decoder(z)\n","out_decoder = Dense(n_in_out, activation=\"sigmoid\")  # 後で使用\n","y = out_decoder(h_decoder)\n","\n","# VAEのモデルを生成\n","model_vae = Model(x, y)\n","\n","# 損失関数\n","# ---- ここから損失関数ののコードを書く ---\n","\n","\n","vae_loss = \n","# ---- ここまで ---\n","\n","model_vae.add_loss(vae_loss)\n","model_vae.compile(optimizer=\"rmsprop\")\n","model_vae.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g1p6KSIjkP7u","colab_type":"text"},"source":["## 学習\n","構築したVAEのモデルを使って、学習を行います。  \n","入力を再現するように学習するので、正解は必要ありません。"]},{"cell_type":"code","metadata":{"id":"RNKvkX9x2Cma","colab_type":"code","colab":{}},"source":["model_vae.fit(x_train,\n","              shuffle=True,\n","              epochs=epochs,\n","              batch_size=batch_size,\n","              validation_data=(x_test, None))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yjeua4AOkebA","colab_type":"text"},"source":["## 潜在空間の可視化\n","潜在変数を平面にプロットし、潜在空間を可視化します。  "]},{"cell_type":"code","metadata":{"id":"QtGpxQGz5OxB","colab_type":"code","colab":{}},"source":["# 潜在変数を得るためのモデル\n","encoder = Model(x, z)\n","\n","# 訓練データから作った潜在変数を2次元プロット\n","z_train = encoder.predict(x_train, batch_size=batch_size)\n","plt.figure(figsize=(6, 6))\n","plt.scatter(z_train[:, 0], z_train[:, 1], c=t_train)  # ラベルを色で表す\n","plt.title(\"Train\")\n","plt.colorbar()\n","plt.show()\n","\n","# テストデータを入力して潜在空間に2次元プロットする 正解ラベルを色で表示\n","z_test = encoder.predict(x_test, batch_size=batch_size)\n","plt.figure(figsize=(6, 6))\n","plt.scatter(z_test[:, 0], z_test[:, 1], c=t_test)\n","plt.title(\"Test\")\n","plt.colorbar()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l4xByaXHpBOh","colab_type":"text"},"source":["訓練データ、テストデータともに、\n","同じような領域に各数字の潜在変数が分布していることが確認できます。"]},{"cell_type":"markdown","metadata":{"id":"DV_Vous8QY9O","colab_type":"text"},"source":["## 画像の生成\n","訓練済みのVAEを使って、画像を生成します。  \n","潜在変数を連続的に変化させて、生成される画像がどのように変化するのかを確認します。"]},{"cell_type":"code","metadata":{"id":"f9lbH_9cQkMH","colab_type":"code","colab":{}},"source":["# 画像の生成器\n","input_decoder = Input(shape=(n_z,))\n","h_decoder = mid_decoder(input_decoder)\n","y = out_decoder(h_decoder)\n","generator = Model(input_decoder, y)\n","\n","# 画像を並べる設定\n","n = 16  # 手書き文字画像を16x16並べる\n","image_size = 28\n","matrix_image = np.zeros((image_size*n, image_size*n))  # 全体の画像\n","\n","# 潜在変数\n","z_1 = np.linspace(5, -5, n)  # 各行\n","z_2 = np.linspace(-5, 5, n)  # 各列\n","\n","#  潜在変数を変化させて画像を生成\n","for i, z1 in enumerate(z_1):\n","    for j, z2 in enumerate(z_2):\n","        decoded = generator.predict(np.array([[z2, z1]]))  # x軸、y軸の順に入れる\n","        image = decoded[0].reshape(image_size, image_size)\n","        matrix_image[i*image_size : (i+1)*image_size, j*image_size: (j+1)*image_size] = image\n","\n","plt.figure(figsize=(10, 10))\n","plt.imshow(matrix_image, cmap=\"Greys_r\")\n","plt.tick_params(labelbottom=False, labelleft=False, bottom=False, left=False)  # 軸目盛りのラベルと線を消す\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e8Mwqhq8XZnb","colab_type":"text"},"source":["# 解答例\n","以下は解答例です。  \n","どうしてもわからない時のみ、参考にしましょう。  "]},{"cell_type":"code","metadata":{"id":"q0z051MMbMzj","colab_type":"code","colab":{}},"source":["from keras.models import Model\n","from keras import metrics  # 評価関数\n","from keras.layers import Input, Dense, Lambda\n","from keras import backend as K  # 乱数の発生に使用\n","\n","# 潜在変数をサンプリングするための関数\n","def z_sample(args):\n","    mu, log_var = args  # 潜在変数の平均値と、分散の対数\n","    # ---- ここからReparametrization Trickのコードを書く ---\n","    epsilon = K.random_normal(shape=K.shape(log_var), mean=0, stddev=1)\n","    return mu + epsilon * K.exp(log_var / 2)\n","    # ---- ここまで ---\n","\n","# VAEのネットワーク構築\n","x = Input(shape=(n_in_out,))\n","h_encoder = Dense(n_mid, activation=\"relu\")(x)\n","\n","mu = Dense(n_z)(h_encoder)\n","log_var = Dense(n_z)(h_encoder)\n","z = Lambda(z_sample, output_shape=(n_z,))([mu, log_var])\n","\n","mid_decoder = Dense(n_mid, activation=\"relu\")  # 後で使用\n","h_decoder = mid_decoder(z)\n","out_decoder = Dense(n_in_out, activation=\"sigmoid\")  # 後で使用\n","y = out_decoder(h_decoder)\n","\n","# VAEのモデルを生成\n","model_vae = Model(x, y)\n","\n","# 損失関数\n","# ---- ここから損失関数ののコードを書く ---\n","rec_loss = n_in_out * metrics.binary_crossentropy(x, y)\n","reg_loss = - 0.5 * K.sum(1 + log_var - K.square(mu) - K.exp(log_var), axis=-1)\n","vae_loss = K.mean(rec_loss + reg_loss)\n","# ---- ここまで ---\n","\n","model_vae.add_loss(vae_loss)\n","model_vae.compile(optimizer=\"rmsprop\")\n","model_vae.summary()"],"execution_count":null,"outputs":[]}]}